<!DOCTYPE html>
<html lang="en">

<head>
    <!-- ---------- basic meta & fonts ---------- -->
    <meta charset="UTF-8">
    <title>Seeing More with Less: Human-Like Representations in Vision Models</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- ---------- icons / mathjax ---------- -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- ---------- in-place copy of original stylesheet (abridged for clarity) ---------- -->
    <style>
        :root{color-scheme:light;}
        body{font-family:Arial, sans-serif;line-height:1.5;margin:0;padding:0;color:black;background-size:contain;background-attachment:fixed;background-position:center;direction:ltr;}
        .hero{text-align:center;padding:50px 0;background-color:#fff;border-bottom-left-radius:20px;border-bottom-right-radius:20px;}
        .hero h1{font-size:3em;margin:0.2em 0;font-family:'Inter',sans-serif;color:#76b900;font-weight:700;}
        .hero h2{font-size:2.2em;margin:0.2em 0;font-weight:normal;line-height:1.35;}
        .hero p{font-size:1.4em;margin-bottom:1em;}
        .button{display:inline-flex;align-items:center;padding:10px 20px;margin:5px;font-size:1.1em;color:white;background-color:rgb(246,172,102);border-radius:30px;text-decoration:none;transition:background-color .3s;}
        .button:hover{background-color:rgb(236,162,92);}
        .button .icon{margin-right:8px;}
        .button .icon img{height:1em;width:auto;vertical-align:middle;}

        /* --- description blocks --- */
        .description,.description_noborder{font-family:Arial, sans-serif;font-size:17px;line-height:1.47;color:#333;letter-spacing:-.022em;font-weight:400;background-color:#fff;padding:20px 0;text-align:center;}
        .description{border-top-left-radius:20px;border-top-right-radius:20px;box-shadow:2px 4px 12px #00000054;}
        .description-content{max-width:65%;margin:0 auto;padding:20px;border-radius:18px;}
        .description-content h1{color:black;font-size:2.1em;line-height:1.2;font-weight:600;text-align:center;margin:1em 0 .4em;font-family:'Inter',sans-serif;}
        .description-content h2{color:black;font-size:1.5em;line-height:1.125;font-weight:580;text-align:left;margin:.83em 0;}
        .description-content p{font-size:1.1em;text-align:left;}

        /* --- helpers --- */
        .inserted-image{max-width:80%;height:auto;margin:30px auto 10px auto;display:block;border-radius:10px;box-shadow:2px 2px 10px 3px #00000030;background-color:white;padding:10px;}
        .comparison-container{display:flex;align-items:center;justify-content:center;gap:20px;margin:30px auto;flex-wrap:wrap;}
        .comparison-item{flex:1;text-align:center;min-width:300px;}
        .comparison-item img{max-width:100%;height:auto;border-radius:10px;background-color:white;}
        .comparison-vs{font-size:1.5em;font-weight:bold;color:#76b900;align-self:center;margin:0 5px 0 20px;}

        /* --- footer & modal unchanged --- */
        .footer{background-color:#f5f5f5;box-shadow:2px 4px 12px #00000054;color:#333;padding:20px;text-align:center;margin-top:-20px;border-top-left-radius:20px;border-top-right-radius:20px;}
        .footer a{color:dodgerblue;text-decoration:none;}

        @media(max-width:1024px){.description-content{max-width:92%;}.inserted-image{max-width:95%;}}
    </style>
</head>

<body>
<!-- ===================================================================== -->
<!-- ================================ HERO =============================== -->
<!-- ===================================================================== -->
<div class="hero">
    <h1>Seeing More with Less:</h1>
    <h2>Human-Like Representations in Vision Models</h2>

    <!-- authors -->
    <div style="margin-top:30px;text-align:center;">
        <p style="font-size:1.4em;margin-bottom:5px;width:80%;margin:auto">
            <a href="#" style="color:#6f6f6f;text-decoration:none;">Andrey Gizdov</a><sup style="font-size:.6em;">1,2</sup>&nbsp;&nbsp;
            <a href="#" style="color:#6f6f6f;text-decoration:none;">Shimon Ullman</a><sup style="font-size:.6em;">1,3</sup>&nbsp;&nbsp;
            <a href="#" style="color:#6f6f6f;text-decoration:none;">Daniel Harari</a><sup style="font-size:.6em;">1,3</sup>
        </p>
    </div>

    <!-- institute logos (placeholder PNGs) -->
    <div style="overflow:hidden;background-color:#fff;">
        <div class="logo" style="padding-top:30px;">
            <div style="position:relative;display:inline-block;margin:0 15px;"><sup style="position:absolute;top:-5px;left:-8px;font-size:0.6em;color:#666;">1</sup>
                <img src="assets/weizmann_logo.png" alt="Weizmann Institute" style="height:40px;">
            </div>
            <div style="position:relative;display:inline-block;margin:0 15px;"><sup style="position:absolute;top:-5px;left:-8px;font-size:0.6em;color:#666;">2</sup>
                <img src="assets/harvard_logo.png" alt="Harvard University" style="height:50px;">
            </div>
            <div style="position:relative;display:inline-block;margin:0 15px;"><sup style="position:absolute;top:-5px;left:-8px;font-size:0.6em;color:#666;">3</sup>
                <img src="assets/mit_logo.png" alt="MIT" style="height:40px;">
            </div>
        </div>
    </div>

    <!-- conference badge -->
    <div style="text-align:center;margin-top:20px;margin-bottom:20px;font-size:1.5em;color:#76b900;font-weight:bold;">
        CVPR 2025 • Spotlight
    </div>

    <!-- buttons -->
    <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.pdf"
   class="button">
  <span class="icon"><i class="ai ai-arxiv"></i></span>
  <span>Paper&nbsp;(PDF)</span> </a> 
    <a href="https://www.youtube.com/watch?v=OeOXkVduwWQ" class="button">
  <span class="icon"><i class="videocam"></i></span>  <span>Video Presentation</span> </a>
    <a href="#" class="button"><span class="icon"><i class="fab fa-github"></i></span><span>Code (coming soon)</span></a>
</div>

<!-- ======================= CONTRIBUTION BULLETS ======================= -->
<section class="description_noborder">
    <div class="description-content">
        <p style="margin-bottom:20px;display:flex;align-items:flex-start;">
            <img src="assets/eye_icon.svg" width="45" height="45" style="flex-shrink:0;margin-right:25px;margin-top:4px;">
            <span><b>Foveated sampling boosts accuracy</b> of leading LMMs (BLIP-2, LLaVA, ViLT, MDETR, InstructBLIP) by up to 2.7 % under identical pixel budgets.</span>
        </p>
        <p style="margin-bottom:20px;display:flex;align-items:flex-start;">
            <img src="assets/dim_returns_icon.png" width="40" height="40" style="flex-shrink:0;margin-right:30px;margin-top:-8px;">
            <span><b>Only 3 % pixels → 80 % performance.</b> Scaling curves reveal strong diminishing returns of resolution.</span>
        </p>
        <p style="margin-bottom:20px;display:flex;align-items:flex-start;">
            <img src="assets/global_attn_icon.png" width="40" height="40" style="flex-shrink:0;margin-right:30px;margin-top:4px;">
            <span>Variable sampling induces <b>human-like global self-attention</b> &amp; <b>resolution-selective neurons</b> in CNNs and transformers.</span>
        </p>
        <p style="margin-bottom:0;display:flex;align-items:flex-start;">
            <img src="assets/bandwidth_icon.png" width="40" height="40" style="flex-shrink:0;margin-right:30px;margin-top:0px;">
            <span>Enables vision on <em>5–10 ×</em> lower bandwidth for UAVs, IoT cameras &amp; wearables while retaining 80–95 % accuracy.</span>
        </p>
    </div>
</section>

<!-- ===================================================================== -->
<!-- ========================== STORYLINE SECTIONS ======================= -->
<!-- ===================================================================== -->
<section class="description">

    <!-- Abstract section (unchanged) -->
    <div class="description-content">
        <h1>Abstract – in plain English</h1>
        <p>
            The human eye keeps only a thumb-nail-sized region in razor-sharp focus while resolution falls towards the periphery.
            This mechanism is known as foveation and is a clever way to reduce the amount of visual information our brain needs to process while still capturing <b>fine details + a wide field-of-view</b>.
            Modern vision–language models, however, treat every pixel equally, incurring extra compute and memory for information that often matters less.
            <br><br>
            We explore a simple thought experiment: keep the total pixel count unchanged, but concentrate pixels around a chosen fixation point
            and reduce them in the periphery, mirroring the retina’s layout.
        </p>

        <!-- quick visual side-by-side -->
        <div class="comparison-container">
            <div class="comparison-item">
                <img src="assets/foveated_sampling_on_img.png" alt="Foveated Sampling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Foveated Sampling</div>
            </div>
            <div class="comparison-vs">VS</div>
            <div class="comparison-item">
                <img src="assets/uniform_sampling_on_img.png" alt="Uniform Downscaling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Uniform Downscaling</div>
            </div>
        </div>

        <p>

            Given that the two (motorbike) representations above both comprise the same number of pixels, just distributed differently, we simply ask
            which visual representation is better for vision models. 
            The result is measurable and task-agnostic: with the same 3 % pixel budget VLMs gain <b>+2.7 %</b> accuracy on visual question answering
            and <b>+2.2 %</b> on object detection. In short, variable sampling delivers consistent gains even without re-training models.
        </p>
    </div>

    <!-- -------------- Why foveation? -------------- -->
    <div class="description-content">
        <h1>Why foveation?</h1>
        <p>
            The image bellow is too large to (1) be transmitted over most network channels in real time; (2) too large to store in memory; 
            (3) too large to process via VLMs.
            
            <!-- Images with captions -->
            <div style="text-align: center; margin: 20px 0;">
                <img src="assets/image_supermarket_large.png" alt="" class="inserted-image" style="max-width: 100%;">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Full image (14,400px)</div>
            </div>
            <div style="text-align: center; margin: 20px 0;">
                <img src="assets/image_supermarket_small.jpg" alt="" class="inserted-image" style="max-width: 100%;">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Downscaled image (1,024px)</div>
            </div>
            
            While downscaling the image solves the problem, it looses critical details. Existing methods such as
            dynamic tokenization or token merging can be used as workarounds, but they still require the full image to begin with.
            This is a problem when the bottleneck in a computer vision pipeline is the network bandwidth, which is the case in long-range
            communication devices such as UAVs, IoT cameras and wearables. Then the question becomes: which pixels to send to the reciever?
            We show that, even without training, one is better off sending pixels in a variable manner, rather than downscaling the image.
        </p>
    </div>

    <!-- -------------- Sampling maps & reconstruction -------------- -->
    <div class="description-content">
        <h1>How we build the sampling maps</h1>
        <p>
            All three schemes live on the same log-polar coordinate transform  
            \[
              (r,\theta)=\Bigl(\log\!\bigl(\sqrt{(x-x_f)^2+(y-y_f)^2}\bigr),\;
              \arctan\!\frac{y-y_f}{x-x_f}\Bigr),
            \]
            where \((x_f,y_f)\) is the fixation point.  
            For <b>S<sub>var</sub></b> we allocate a fixed quota of samples to concentric annuli whose area <em>decreases linearly</em> with radius—mimicking the widening receptive fields of retinal ganglion cells.
            For <b>S<sub>uni</sub></b> each annulus contains the same number of samples, yielding an even pixel density.
        </p>
        <p>
            We mask the original image with the sampling maps, \(I(x,y)\), and then reconstruct a full-resolution frame with bilinear
            interpolation.
            \[
              \hat I = \mathcal I\bigl(I(x,y)\cdot S(x,y)\bigr),
            \]
            ensuring that <em>model architectures remain unchanged</em>.  
            At 3 % density a 720 p frame shrinks from 1.3 MB to ~45 kB on the wire.
        </p>
        <!-- quick visual side-by-side -->
        <div class="comparison-container">
            <div class="comparison-item">
                <img src="assets/var_rf_layout.png" alt="Foveated Sampling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Variable size receptive fields</div>
            </div>
            <div class="comparison-item">
                <img src="assets/uni_rf_layout.png" alt="Uniform Downscaling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Uniform size receptive fields</div>
            </div>
        </div>
        <div class="comparison-container">
            <div class="comparison-item">
                <img src="assets/var_10d_sample_layout_dilate.png" alt="Foveated Sampling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Variable sampling</div>
            </div>
            <div class="comparison-vs">VS</div>
            <div class="comparison-item">
                <img src="assets/uni_10d_sample_layout_dilate.png" alt="Uniform Downscaling">
                <div style="margin-top:10px;font-weight:bold;color:#333;">Uniform sampling</div>
            </div>
        </div>
    </div>

    <!-- -------------- Experimental matrix -------------- -->
    <div class="description-content">
        <h1>Main experiments with 3% pixel budget</h1>
        <p>
            We evaluate three vision tasks across seven representative models:
        </p>
        <ul style="text-align:left;font-size:1.12em;line-height:1.5;">
            <li><b>Visual Question Answering</b> – ViLT-B/32, BLIP-2, InstructBLIP, LLaVA-v1.5</li>
            <li><b>Referring-expression Object Detection</b> – MDETR-ResNet101 • DETR-R101 • Mask R-CNN-R101</li>
            <li><b>Free-text Grounding</b> – (qualitative) BLIP-2 on SEED-Bench</li>
        </ul>
    </div>

    <!-- -------------- VQA results -------------- -->
    <div class="description-content">
        <h1>Visual-QA results (3 % pixel budget)</h1>
        <p>
            Foveated inputs outperform uniform down-scaling across all benchmarks.  
            <em>STable 2–6</em> (supplementary) give full breakdowns by question category; a snapshot for ViLT and BLIP-2 on VQAv2 is shown below.
        </p>
    </div>

    <!-- -------------- Fixation-independence -------------- -->
    <div class="description-content">
        <h2>Does the fixation point matter?</h2>
        <p>
            We shift the fovea 100 px toward each corner. Accuracy varies by &lt; 0.5 % (<em>STable 1</em>).  
            Conclusion: performance stems from variable density—not from a lucky fixation bias.
        </p>
        
        <div style="overflow-x: auto; margin: 20px 0;">
            <table style="width: 100%; border-collapse: collapse; font-size: 0.9em; margin: 20px auto; background-color: white; box-shadow: 2px 2px 10px 3px #00000030; border-radius: 10px;">
                <caption style="caption-side: top; font-weight: bold; margin-bottom: 10px; color: #333;">
                    STable 1. Performance accuracy comparison between variable central fixation and corner fixations across different models and datasets
                </caption>
                <thead>
                    <tr style="background-color: #f8f9fa; border-bottom: 2px solid #dee2e6;">
                        <th style="padding: 12px; text-align: left; border-right: 1px solid #dee2e6;">Model</th>
                        <th style="padding: 12px; text-align: left; border-right: 1px solid #dee2e6;">#Total Params</th>
                        <th style="padding: 12px; text-align: left; border-right: 1px solid #dee2e6;">Dataset</th>
                        <th style="padding: 12px; text-align: center; border-right: 1px solid #dee2e6;">Variable</th>
                        <th style="padding: 12px; text-align: center; border-right: 1px solid #dee2e6;">Variable Std.</th>
                        <th style="padding: 12px; text-align: center;">Uniform</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">MDETR-ResNet101-RoBERTa</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">169M</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">GQA</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">46.79%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.01%</td>
                        <td style="padding: 10px; text-align: center;">44.13%</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6; background-color: #f8f9fa;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">BLIP-2-FlanT5XL</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">3.4B</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">GQA</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">42.27%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.21%</td>
                        <td style="padding: 10px; text-align: center;">40.72%</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">BLIP-2-FlanT5XL</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">3.4B</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">VQAv2</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">57.89%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.46%</td>
                        <td style="padding: 10px; text-align: center;">56.19%</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6; background-color: #f8f9fa;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">InstructBLIP-FlanT5XL</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">4B</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">VQAv2</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">66.37%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.56%</td>
                        <td style="padding: 10px; text-align: center;">66.48%</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">ViLT-B/32</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">87.4M</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">VQAv2</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">64.90%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.82%</td>
                        <td style="padding: 10px; text-align: center;">63.01%</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">LLaVa-v1.5</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">13B</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">VQAv2</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">65.91%</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">±0.75%</td>
                        <td style="padding: 10px; text-align: center;">65.14%</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <!-- -------------- Object detection -------------- -->
    <div class="description-content">
        <h1>Object detection &amp; segmentation</h1>
        <p>
            On GQA we observe a consistent +2 – 3 pp gain in mAP<sub>50:95</sub>.  
            To isolate resolution effects we also evaluate only those objects whose masks cover an <em>equal number of samples</em> in both schemes (sample-equalised COCO subset).  
            The table below shows that variable sampling still edges out uniform, especially for small objects.
        </p>
        
        <div style="overflow-x: auto; margin: 20px 0;">
            <table style="width: 100%; border-collapse: collapse; font-size: 0.9em; margin: 20px auto; background-color: white; box-shadow: 2px 2px 10px 3px #00000030; border-radius: 10px;">
                <caption style="caption-side: top; font-weight: bold; margin-bottom: 10px; color: #333;">
                    Object Detection Results: Performance comparison across different sampling strategies
                </caption>
                <thead>
                    <tr style="background-color: #f8f9fa; border-bottom: 2px solid #dee2e6;">
                        <th style="padding: 12px; text-align: left; border-right: 1px solid #dee2e6;">Model</th>
                        <th style="padding: 12px; text-align: left; border-right: 1px solid #dee2e6;">Sampling</th>
                        <th style="padding: 12px; text-align: center; border-right: 1px solid #dee2e6;">AR</th>
                        <th style="padding: 12px; text-align: center; border-right: 1px solid #dee2e6;">ARS</th>
                        <th style="padding: 12px; text-align: center; border-right: 1px solid #dee2e6;">ARM</th>
                        <th style="padding: 12px; text-align: center;">ARL</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">DETR-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Baseline</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">54.8</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">15.3</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">51.9</td>
                        <td style="padding: 10px; text-align: center;">72.0</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6; background-color: #f8f9fa;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">DETR-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Uniform</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">37.1</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">1.2</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">28.0</td>
                        <td style="padding: 10px; text-align: center;">51.6</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">DETR-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">Variable</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">38.5</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">2.1</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">31.0</td>
                        <td style="padding: 10px; text-align: center; font-weight: bold; color: #76b900;">54.7</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6; background-color: #f8f9fa;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Mask RCNN-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Baseline</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">52.5</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">25.6</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">49.7</td>
                        <td style="padding: 10px; text-align: center;">64.2</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #dee2e6;">
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Mask RCNN-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Uniform</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">34.7</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">1.6</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6;">27.7</td>
                        <td style="padding: 10px; text-align: center;">48.2</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6;">Mask RCNN-R101</td>
                        <td style="padding: 10px; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">Variable</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">36.9</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">3.4</td>
                        <td style="padding: 10px; text-align: center; border-right: 1px solid #dee2e6; font-weight: bold; color: #76b900;">31.7</td>
                        <td style="padding: 10px; text-align: center; font-weight: bold; color: #76b900;">48.6</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <p>
            <em>STable 7</em> (supplementary) provides additional detailed breakdowns by object size and category.
        </p>
    </div>

    <!-- -------------- Representational analysis -------------- -->
    <div class="description-content">
        <h1>Human-like representations</h1>
        <p>
            <strong>Transformers.</strong> Average attention-distance grows by <b>30 %</b> in foveated inputs, paralleling the long-range lateral connections of V1.  
            <strong>CNNs.</strong> A permutation test on 5 k tensors (see §5 in the supplement) rejects H<sub>0</sub> (p &lt; 10<sup>-3</sup>): neurons specialise for high- vs low-resolution inputs.
        </p>
        <img src="assets/interpretability_fig.png" alt="Attention maps & neuron specialisation"
             class="inserted-image">
    </div>

    <!-- -------------- Edge compute -------------- -->
    <div class="description-content">
        <h1>Edge-compute impact</h1>
        <p>
            A LoRa-equipped drone is capped at ≈ 70 kbit s<sup>-1</sup>.  
            Our 3 % pipeline streams a 720 p scene at ≈ 45 kbit s<sup>-1</sup> while retaining ~80 % of full-resolution accuracy—enabling minimum bandwidth AI control.
        </p>
    </div>

    <!-- -------------- Limitations & future work -------------- -->
    <div class="description-content">
        <h1>Limitations &amp; future work</h1>
        <ul style="text-align:left;font-size:1.12em;line-height:1.55;">
            <li>Architectures are frozen; end-to-end training on sparse tensors may unlock further efficiency.</li>
            <li>Single fixation per image; real eyes saccade 3–4 Hz. Integrating learned fixation policies is a next step.</li>
            <li>Current VLMs inherit full-res pre-training; retraining from scratch on foveated data is less compute-heavy.</li>
        </ul>
    </div>
</section>

<!-- ======================= FOOTER ======================= -->
<footer class="footer">
    Built with ♥ on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
</footer>

<!-- ======================= JS: modal viewer (kept) ======================= -->
<script>
/* click-to-zoom for any img with class inserted-image */
document.querySelectorAll('.inserted-image').forEach(img=>{
    img.addEventListener('click',()=>{
        if(!document.getElementById('modal')){
            const m=document.createElement('div');m.id='modal';m.style.cssText='position:fixed;inset:0;display:flex;align-items:center;justify-content:center;background:rgba(0,0,0,0.9);z-index:99;';
            m.innerHTML='<img id="modal-img" style="max-width:90%;max-height:90%;object-fit:contain;border-radius:8px;">';
            m.onclick=()=>m.remove();document.body.appendChild(m);
        }
        document.getElementById('modal-img').src=img.src;
        document.getElementById('modal').style.display='flex';
    });
});
</script>
</body>
</html>
